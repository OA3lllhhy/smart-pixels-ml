{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e0e8205-29bf-4c66-bc22-a1731bb28834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PARETO FRONT STUDIES NOTEBOOK #1 ===\n",
    "# Author: Ana Sofia Calle Mu√±oz\n",
    "\n",
    "# This notebook allows you to perform a Pareto analysis for a specific scheduler over a given number of trials and epochs.\n",
    "# Losses used: NLL and a sum of standard deviations sigma regularizer.\n",
    "# !!! Throughout the notebooks, look for comments including \"==\" at the beginning, as they need to be modified on your end.\n",
    "\n",
    "# == Before running:\n",
    "# Download the 4 notebooks and schedulers.py file & upload to your cluster.\n",
    "# Make sure you have the latest OptimizedDataGenerator_v2 python file.\n",
    "# Upload the model you want to work with. This nb is set to work with the Conv2D Max model & 16x16 sensor size.\n",
    "# Modify the train & validation TFrecords folder paths. This nb works with dataset_3sr filtered with labels.\n",
    "# Modify the \"intermediate_dir\" path, that is where your results will save.\n",
    "# Modify the sample hyperparameter ranges as you like. They are in the run_trials funtion.\n",
    "\n",
    "# == Run:\n",
    "# Go to the last block and look for the run_trials call. Set a scheduler, experiment name, number of trials and epochs. That's it!\n",
    "\n",
    "# Any questions you have, you can reach out on the FastML slack as Ana Sofia Calle, or callea@purdue.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc98d975-f552-42b0-bf70-8b9c62ececc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-26 13:09:15.549499: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-26 13:09:15.562621: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-26 13:09:15.615005: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-26 13:09:15.615041: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-26 13:09:15.616815: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-26 13:09:15.625148: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-26 13:09:15.625843: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-26 13:09:22.131986: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/hep/hl2822/smart-pixels-ml/smart-pixel/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from qkeras import *\n",
    "\n",
    "from keras.utils import Sequence\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import EarlyStopping, Callback, LambdaCallback\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy\n",
    "\n",
    "from tensorflow.keras.metrics import Mean\n",
    "\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "\n",
    "from OptimizedDataGenerator_v2 import OptimizedDataGenerator\n",
    "from schedulers import *\n",
    "import pickle\n",
    "from models_16x16.models import *\n",
    "\n",
    "pi = 3.14159265359\n",
    "\n",
    "maxval=1e9\n",
    "minval=1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f177bc1-468a-4add-bd84-1151e0ebdd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c59055ee-2084-43ce-bf3b-a298cf903d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==TFrecords paths\n",
    "# I recommend using contained datasets for better results\n",
    "\n",
    "tfrecords_dir_train = \"/home/hep/hl2822/smart-pixels-ml/tfrecords_3e778b82/tfrecords_train_3e778b82\"\n",
    "tfrecords_dir_validation = \"/home/hep/hl2822/smart-pixels-ml/tfrecords_3e778b82/tfrecords_validation_3e778b82\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "005e7cb5-99e1-4851-9b2f-abc49d9dcc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss: NLL and a sum of standard deviations sigma regularizer (you can modify the regularizer).\n",
    "\n",
    "current_reg_weight = tf.Variable(0.0, trainable=False, dtype=tf.float32, name='reg_weight')\n",
    "\n",
    "def custom_loss(y, p_base, minval=1e-9, maxval=1e9, scale = 512):\n",
    "\n",
    "    reg_weight = current_reg_weight\n",
    "    \n",
    "    p = p_base\n",
    "    \n",
    "    mu = p[:, 0:8:2]\n",
    "    \n",
    "    # creating each matrix element in 4x4\n",
    "    Mdia = minval + tf.math.maximum(p[:, 1:8:2], 0.0)\n",
    "    Mcov = p[:,8:]\n",
    "    \n",
    "    # placeholder zero element\n",
    "    zeros = tf.zeros_like(Mdia[:,0])\n",
    "    \n",
    "    # assembles scale_tril matrix\n",
    "    row1 = tf.stack([Mdia[:,0],zeros,zeros,zeros])\n",
    "    row2 = tf.stack([Mcov[:,0],Mdia[:,1],zeros,zeros])\n",
    "    row3 = tf.stack([Mcov[:,1],Mcov[:,2],Mdia[:,2],zeros])\n",
    "    row4 = tf.stack([Mcov[:,3],Mcov[:,4],Mcov[:,5],Mdia[:,3]])\n",
    "\n",
    "    scale_tril = tf.transpose(tf.stack([row1,row2,row3,row4]),perm=[2,0,1])\n",
    "\n",
    "    dist = tfp.distributions.MultivariateNormalTriL(loc = mu, scale_tril = scale_tril) \n",
    "    \n",
    "    likelihood = dist.prob(y)  \n",
    "    likelihood = tf.clip_by_value(likelihood,minval,maxval)\n",
    "\n",
    "    NLL = -1*tf.math.log(likelihood)\n",
    "\n",
    "    cov_matrix = tf.matmul(scale_tril, tf.transpose(scale_tril, [0, 2, 1])) \n",
    "    variances = tf.linalg.diag_part(cov_matrix)\n",
    "    stds = tf.sqrt(variances + minval)\n",
    "\n",
    "    sigma_regularizer_1 = tf.reduce_sum(stds, axis=1)\n",
    "\n",
    "    batch_size = tf.shape(y)[0]\n",
    "    \n",
    "    track_loss_values(NLL, sigma_regularizer_1)\n",
    "\n",
    "    total_loss = NLL + (sigma_regularizer_1 * reg_weight)\n",
    "    \n",
    "    return tf.keras.backend.sum(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70926e82-659c-4ee4-bcc1-6b6912cb78a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduler that scales and saves the nll and reg mean loss\n",
    "class EpochValidationSaver(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, val_data, reg_weight):\n",
    "        super().__init__()\n",
    "        self.val_data = val_data\n",
    "        self.reg_weight = reg_weight\n",
    "        self.intermediate_points = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        reset_loss_trackers()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for x_val, y_val in self.val_data:\n",
    "            y_pred = self.model(x_val, training=False)\n",
    "\n",
    "            mu = y_pred[:, 0:8:2]\n",
    "            Mdia = 1e-9 + tf.math.maximum(y_pred[:, 1:8:2], 0.0)\n",
    "            Mcov = y_pred[:, 8:]\n",
    "\n",
    "            zeros = tf.zeros_like(Mdia[:, 0])\n",
    "            row1 = tf.stack([Mdia[:, 0], zeros, zeros, zeros], axis=1)\n",
    "            row2 = tf.stack([Mcov[:, 0], Mdia[:, 1], zeros, zeros], axis=1)\n",
    "            row3 = tf.stack([Mcov[:, 1], Mcov[:, 2], Mdia[:, 2], zeros], axis=1)\n",
    "            row4 = tf.stack([Mcov[:, 3], Mcov[:, 4], Mcov[:, 5], Mdia[:, 3]], axis=1)\n",
    "            scale_tril = tf.stack([row1, row2, row3, row4], axis=1)\n",
    "\n",
    "            dist = tfp.distributions.MultivariateNormalTriL(loc=mu, scale_tril=scale_tril)\n",
    "            likelihood = tf.clip_by_value(dist.prob(y_val), 1e-9, 1e9)\n",
    "\n",
    "            NLL = -tf.math.log(likelihood)\n",
    "            cov_matrix = tf.matmul(scale_tril, tf.transpose(scale_tril, [0, 2, 1]))\n",
    "            stds = tf.sqrt(tf.linalg.diag_part(cov_matrix) + 1e-9)\n",
    "            sigma_regularizer_1 = tf.reduce_sum(stds, axis=1)\n",
    "\n",
    "            track_loss_values(NLL, sigma_regularizer_1)\n",
    "\n",
    "        # Calculates total samples in val_data\n",
    "        num_val_samples = sum(x.shape[0] for x, _ in self.val_data)\n",
    "\n",
    "        # Obtain mean metrics per sample (accumulates values batch by batch and computes the average per sample)\n",
    "        # This approach is not affected by batch size/shuffling of the dataset.\n",
    "        metrics = get_loss_metrics()\n",
    "        nll_mean = metrics['nll_mean']\n",
    "        reg_mean = metrics['reg_mean']\n",
    "\n",
    "        nll_total = nll_mean * num_val_samples\n",
    "        reg_total = reg_mean * num_val_samples\n",
    "        reg_weight_value = float(self.reg_weight.numpy())\n",
    "        total_loss = nll_total + reg_weight_value * reg_total\n",
    "\n",
    "        self.keras_style_val_loss = logs.get(\"val_loss\")\n",
    "        # scale the losses\n",
    "        logs['val_loss'] = total_loss / num_val_samples\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] NLL_mean={nll_mean:.6f}, REG_mean={reg_mean:.6f}\")\n",
    "        self.intermediate_points.append((nll_mean, reg_mean))\n",
    "\n",
    "def get_epoch_callback(validation_generator, reg_weight):\n",
    "    saver = EpochValidationSaver(val_data=validation_generator, reg_weight=reg_weight)\n",
    "    return saver, saver\n",
    "\n",
    "# Scheduler that earlystops trials with non-improving loss\n",
    "class EarlyStopNoImprovement(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, patience=2, min_delta=0.1, threshold=19.0, min_val_loss_to_keep=10.0):\n",
    "        super().__init__()\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.threshold = threshold\n",
    "        self.min_val_loss_to_keep = min_val_loss_to_keep\n",
    "        self.best_loss = float('inf')\n",
    "        self.wait = 0\n",
    "        self.early_stop_triggered = False\n",
    "        self.bad_trial_due_to_high_loss = False \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = logs.get(\"val_loss\")\n",
    "        if val_loss is None:\n",
    "            return\n",
    "\n",
    "        if val_loss <= self.threshold:\n",
    "            print(f\"val_loss={val_loss:.4f} is below the threshold ({self.threshold}), early stopping is not applied.\")\n",
    "            return\n",
    "\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            print(f\"Improvement detected: val_loss decreased from {self.best_loss:.4f} to {val_loss:.4f}\")\n",
    "            self.best_loss = val_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            print(f\"No significant improvement for {self.wait} epochs (val_loss={val_loss:.4f})\")\n",
    "\n",
    "        if self.wait >= self.patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} due to stagnation (val_loss={val_loss:.4f})\")\n",
    "            self.early_stop_triggered = True\n",
    "            if self.best_loss > self.min_val_loss_to_keep:\n",
    "                self.bad_trial_due_to_high_loss = True\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "205f0552-a9e8-47ec-8bdf-4f7f6a29d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to choose the scheduler you want to study\n",
    "# You can add more in the schedulers.py file and add an extra elif here\n",
    "\n",
    "def get_scheduler(scheduler_type, reg_weight_var, **kwargs):\n",
    "    if scheduler_type == \"cosine\":\n",
    "        return CosineScheduler(**kwargs, reg_weight_var=reg_weight_var)\n",
    "    elif scheduler_type == \"linear\":\n",
    "        return LinearScheduler(**kwargs, reg_weight_var=reg_weight_var)\n",
    "    elif scheduler_type == \"adaptive\":\n",
    "        return AdaptiveScheduler(**kwargs, reg_weight_var=reg_weight_var)\n",
    "    elif scheduler_type == \"sigmoid\":\n",
    "        return SigmoidScheduler(**kwargs, reg_weight_var=reg_weight_var)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scheduler type: {scheduler_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ed43415-fdc9-4bdf-b906-cd6250e9aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==Folder path where your results will save!!\n",
    "intermediate_dir = \"/home/hep/hl2822/smart-pixels-ml/intermediate_logs\"\n",
    "os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "def objective_manual(trial_id, lambda_init, lambda_final, stop_threshold, experiment_name, scheduler_type, scheduler_kwargs, epochs):\n",
    "    global intermediate_dir\n",
    "    reset_loss_trackers()\n",
    "    current_reg_weight.assign(scheduler_kwargs['start'])\n",
    "\n",
    "    # Create and compile model\n",
    "    input_shape = (16, 16, 2)\n",
    "    model = CreateModel(input_shape, n_filters=5, pool_size=3)\n",
    "    model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=1e-3), loss=custom_loss)\n",
    "\n",
    "    # Data generators\n",
    "    training_generator = OptimizedDataGenerator(\n",
    "        load_from_tfrecords_dir=tfrecords_dir_train,\n",
    "        shuffle=True, seed=13, quantize=True\n",
    "    )\n",
    "    validation_generator = OptimizedDataGenerator(\n",
    "        load_from_tfrecords_dir=tfrecords_dir_validation,\n",
    "        shuffle=False, seed=13, quantize=True\n",
    "    )\n",
    "\n",
    "    # Callbacks\n",
    "    val_callback, saver = get_epoch_callback(validation_generator, reg_weight=current_reg_weight)\n",
    "    \n",
    "    if scheduler_type == \"adaptive\":\n",
    "        valid_scheduler_keys = {\"start\", \"step\", \"patience\"}\n",
    "    else:\n",
    "        valid_scheduler_keys = {\"start\", \"end\", \"max_epochs\", \"step\", \"patience\", \"sharpness\"}\n",
    "    \n",
    "    filtered_kwargs = {k: v for k, v in scheduler_kwargs.items() if k in valid_scheduler_keys}\n",
    "    \n",
    "    scheduler = get_scheduler(\n",
    "        scheduler_type=scheduler_type,\n",
    "        reg_weight_var=current_reg_weight,\n",
    "        **filtered_kwargs\n",
    "    )\n",
    "\n",
    "    bad_loss = EarlyStopNoImprovement(patience=2, min_delta=0.1)\n",
    "\n",
    "    # Training\n",
    "    model.fit(\n",
    "        training_generator,\n",
    "        validation_data=validation_generator,\n",
    "        epochs=scheduler_kwargs.get(\"max_epochs\", epochs),\n",
    "        callbacks=[scheduler, val_callback, bad_loss],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Intermediate points file path\n",
    "    # Thoughout the training of a trial, every nll x reg validation loss point will save. These are called intermediate points.\n",
    "    exp_dir = os.path.join(intermediate_dir, experiment_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    inter_path = os.path.join(exp_dir, f\"trial_{trial_id}_intermediate.pkl\")\n",
    "    \n",
    "    keras_val_loss = val_callback.keras_style_val_loss\n",
    "\n",
    "    # Trial info structure\n",
    "    trial_info = {\n",
    "        \"trial_id\": trial_id,\n",
    "        \"scheduler\": scheduler_type,\n",
    "        **scheduler_kwargs,\n",
    "        \"valid_trial\": False,\n",
    "        \"final_nll\": None,\n",
    "        \"final_reg\": None,\n",
    "        \"final_val_loss\": None,\n",
    "        \"keras_val_loss\": keras_val_loss\n",
    "    }\n",
    "\n",
    "    if getattr(bad_loss, \"bad_trial_due_to_high_loss\", False):\n",
    "        print(f\"üóëÔ∏è Trial {trial_id} discarded due to high loss\")\n",
    "        if os.path.exists(inter_path):\n",
    "            os.remove(inter_path)\n",
    "    else:\n",
    "        with open(inter_path, \"wb\") as f:\n",
    "            pickle.dump(saver.intermediate_points, f)\n",
    "\n",
    "        metrics = get_loss_metrics()\n",
    "        nll, reg = float(metrics[\"nll_mean\"]), float(metrics[\"reg_mean\"])\n",
    "        final_lambda = float(current_reg_weight.numpy())\n",
    "        \n",
    "        MAX_NLL_TO_KEEP = 15.0\n",
    "        if nll > MAX_NLL_TO_KEEP:\n",
    "            print(f\"üóëÔ∏è Trial {trial_id} discarded due to high NLL: {nll:.2f}\")\n",
    "            os.remove(inter_path)\n",
    "            trial_info.update({\"final_nll\": nll, \"final_reg\": reg})\n",
    "        else:\n",
    "            val_loss_final = nll + final_lambda * reg\n",
    "            print(f\"‚úÖ Trial {trial_id} succeeded\")\n",
    "            trial_info.update({\n",
    "                \"valid_trial\": True,\n",
    "                \"final_nll\": nll,\n",
    "                \"final_reg\": reg,\n",
    "                \"final_val_loss\": val_loss_final\n",
    "            })\n",
    "\n",
    "    # Save csv\n",
    "    # Non-improving trials will NOT save their intermediate points & loss info on the csv \n",
    "    csv_path = os.path.join(exp_dir, \"info.csv\")\n",
    "    write_header = not os.path.exists(csv_path)\n",
    "\n",
    "    with open(csv_path, mode=\"a\", newline=\"\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=trial_info.keys())\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(trial_info)\n",
    "\n",
    "    return (trial_info[\"final_nll\"], trial_info[\"final_reg\"]) if trial_info[\"valid_trial\"] else (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dc34585-f62f-4e78-8eb8-fb0fa7ea84b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Training Trial 0 using scheduler=cosine\n",
      "Loading metadata from /home/hep/hl2822/smart-pixels-ml/tfrecords_3e778b82/tfrecords_train_3e778b82/metadata.json\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 49\u001b[0m\n\u001b[1;32m     45\u001b[0m         trial_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# == Modify this block for each study you make \u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# If you want very good pull/truth plots I recommend setting a lot of epochs ~ 500\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mrun_trials\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcosine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgeneral_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 31\u001b[0m, in \u001b[0;36mrun_trials\u001b[0;34m(scheduler_type, experiment_name, num_trials, epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m lambda_final \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m stop_threshold \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 31\u001b[0m nll, reg \u001b[38;5;241m=\u001b[39m \u001b[43mobjective_manual\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_final\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_final\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nll \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     successful_trials \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m, in \u001b[0;36mobjective_manual\u001b[0;34m(trial_id, lambda_init, lambda_final, stop_threshold, experiment_name, scheduler_type, scheduler_kwargs, epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m), loss\u001b[38;5;241m=\u001b[39mcustom_loss)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Data generators\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m training_generator \u001b[38;5;241m=\u001b[39m \u001b[43mOptimizedDataGenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_from_tfrecords_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfrecords_dir_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m validation_generator \u001b[38;5;241m=\u001b[39m OptimizedDataGenerator(\n\u001b[1;32m     21\u001b[0m     load_from_tfrecords_dir\u001b[38;5;241m=\u001b[39mtfrecords_dir_validation,\n\u001b[1;32m     22\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m13\u001b[39m, quantize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Callbacks\u001b[39;00m\n",
      "File \u001b[0;32m~/smart-pixels-ml/OptimizedDataGenerator_v2.py:162\u001b[0m, in \u001b[0;36mOptimizedDataGenerator.__init__\u001b[0;34m(self, dataset_base_dir, batch_size, optimize_batch_size, file_count, labels_list, to_standardize, input_shape, transpose, files_from_end, shuffle, load_from_tfrecords_dir, tfrecords_dir, use_time_stamps, select_contained, seed, quantize, max_workers, label_scale_pctl, norm_pos_pctl, norm_neg_pctl, tail_tol, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfrecords_dir \u001b[38;5;241m=\u001b[39m load_from_tfrecords_dir\n\u001b[1;32m    161\u001b[0m metadata_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfrecords_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfrecord_filenames \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msort(np\u001b[38;5;241m.\u001b[39marray(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfrecords_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.tfrecord\u001b[39m\u001b[38;5;124m\"\u001b[39m))))\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantize \u001b[38;5;241m=\u001b[39m quantize\n",
      "File \u001b[0;32m~/smart-pixels-ml/OptimizedDataGenerator_v2.py:221\u001b[0m, in \u001b[0;36mOptimizedDataGenerator.load_metadata\u001b[0;34m(self, metadata_file_path)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading metadata from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetadata_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(metadata_file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 221\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# Key configurations\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/usr/lib64/python3.9/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib64/python3.9/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/lib64/python3.9/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/usr/lib64/python3.9/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "# Trial runner \n",
    "def run_trials(scheduler_type, experiment_name, num_trials, epochs):\n",
    "    successful_trials = 0\n",
    "    trial_id = 0\n",
    "\n",
    "    while successful_trials < num_trials:\n",
    "        \n",
    "        # ==Sample hyperparameters (randomly selected)\n",
    "        # Modify the ranges you want so you can explore your scheduler's behavior.\n",
    "        # If you add a new scheduler and it has new parameters, be sure to introduce them here\n",
    "        \n",
    "        start_val = random.uniform(0.01, 3.0)\n",
    "        config = {\n",
    "            \"start\": start_val,\n",
    "            \"end\": random.uniform(start_val + 0.01, 8.0),\n",
    "            \"stop_threshold\": random.uniform(-40000.0, -20000.0),\n",
    "            \"max_epochs\": epochs\n",
    "        }\n",
    "        if scheduler_type == \"adaptive\":\n",
    "            config[\"step\"] = random.uniform(0.01, 0.2)\n",
    "            config[\"patience\"] = random.randint(3, 7)\n",
    "        elif scheduler_type == \"sigmoid\":\n",
    "            config[\"sharpness\"] = random.randint(2, 15)\n",
    "\n",
    "        print(f\"\\nüîÅ Training Trial {trial_id} using scheduler={scheduler_type}\")\n",
    "\n",
    "        lambda_init = config[\"start\"]\n",
    "        lambda_final = config.get(\"end\", None)\n",
    "        stop_threshold = config[\"stop_threshold\"]\n",
    "\n",
    "        nll, reg = objective_manual(\n",
    "            trial_id=trial_id,\n",
    "            lambda_init=lambda_init,\n",
    "            lambda_final=lambda_final,\n",
    "            stop_threshold=stop_threshold,\n",
    "            experiment_name=experiment_name,\n",
    "            scheduler_type=scheduler_type,\n",
    "            scheduler_kwargs=config,\n",
    "            epochs=epochs\n",
    "        )\n",
    "\n",
    "        if nll is not None:\n",
    "            successful_trials += 1\n",
    "\n",
    "        trial_id += 1\n",
    "\n",
    "# == Modify this block for each study you make \n",
    "# If you want very good pull/truth plots I recommend setting a lot of epochs ~ 500\n",
    "run_trials(\n",
    "    scheduler_type=\"cosine\",\n",
    "    experiment_name=\"general_test\",\n",
    "    num_trials=3, \n",
    "    epochs=500,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smart-pixel (3.9.21)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
